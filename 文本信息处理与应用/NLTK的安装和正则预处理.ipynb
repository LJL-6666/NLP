{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T12:42:03.929060Z",
     "start_time": "2021-03-08T12:41:42.571375Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T13:41:06.333494Z",
     "start_time": "2021-03-08T13:37:58.507010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\Anaconda3\\anaconda\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"E:\\Anaconda3\\anaconda\\lib\\site-packages\\nltk\\downloader.py\", line 2106, in run\n",
      "    for msg in self.data_server.incr_download(self.items):\n",
      "  File \"E:\\Anaconda3\\anaconda\\lib\\site-packages\\nltk\\downloader.py\", line 623, in incr_download\n",
      "    for msg in self._download_list(info_or_id, download_dir, force):\n",
      "  File \"E:\\Anaconda3\\anaconda\\lib\\site-packages\\nltk\\downloader.py\", line 669, in _download_list\n",
      "    for msg in self.incr_download(item, download_dir, force):\n",
      "  File \"E:\\Anaconda3\\anaconda\\lib\\site-packages\\nltk\\downloader.py\", line 637, in incr_download\n",
      "    for msg in self.incr_download(info.children, download_dir, force):\n",
      "  File \"E:\\Anaconda3\\anaconda\\lib\\site-packages\\nltk\\downloader.py\", line 623, in incr_download\n",
      "    for msg in self._download_list(info_or_id, download_dir, force):\n",
      "  File \"E:\\Anaconda3\\anaconda\\lib\\site-packages\\nltk\\downloader.py\", line 669, in _download_list\n",
      "    for msg in self.incr_download(item, download_dir, force):\n",
      "  File \"E:\\Anaconda3\\anaconda\\lib\\site-packages\\nltk\\downloader.py\", line 643, in incr_download\n",
      "    for msg in self._download_package(info, download_dir, force):\n",
      "  File \"E:\\Anaconda3\\anaconda\\lib\\site-packages\\nltk\\downloader.py\", line 697, in _download_package\n",
      "    os.remove(filepath)\n",
      "PermissionError: [WinError 32] 另一个程序正在使用此文件，进程无法访问。: 'E:\\\\Anaconda3\\\\anaconda\\\\nltk_data\\\\corpora\\\\lin_thesaurus.zip'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T13:57:34.806743Z",
     "start_time": "2021-03-09T13:57:09.797611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T13:57:37.927421Z",
     "start_time": "2021-03-09T13:57:37.923432Z"
    }
   },
   "outputs": [],
   "source": [
    "#布朗语料库示例\n",
    "from nltk.corpus import brown  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T13:58:37.409687Z",
     "start_time": "2021-03-09T13:58:37.223343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T13:58:43.366902Z",
     "start_time": "2021-03-09T13:58:40.465511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57340"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:00:22.801987Z",
     "start_time": "2021-03-09T14:00:20.334286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将字⺟转化为⼩写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:00:34.846630Z",
     "start_time": "2021-03-09T14:00:34.832667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 5 biggest countries by population in 2017 are china, india,united states, indonesia, and brazil.\n"
     ]
    }
   ],
   "source": [
    "input_str = 'The 5 biggest countries by population in 2017 are China, India,United States, Indonesia, and Brazil.'\n",
    "input_str = input_str.lower()\n",
    "print(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 删除数字 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:01:06.520802Z",
     "start_time": "2021-03-09T14:01:06.502819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box A contains  red and  white balls, while Box B contains  red and  blue balls.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "input_str ='Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.'\n",
    "result = re.sub(r'\\d+', '', input_str)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 删除空格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:01:24.981216Z",
     "start_time": "2021-03-09T14:01:24.970210Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a string example'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_str = \" \\t a string example\\t \"\n",
    "input_str = input_str.strip()\n",
    "input_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 删除⽂本中出现的终⽌词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "终⽌词（Stop words） 指的是“a”，“a”，“on”，“is”，“all”等语⾔中最常⻅的词。这些词语没什么特别或\n",
    "重要意义，通常可以从⽂本中删除。⼀般使⽤ Natural Language Toolkit（NLTK） 来删除这些终⽌词，\n",
    "这是⼀套专⻔⽤于符号和⾃然语⾔处理统计的开源库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:01:57.689348Z",
     "start_time": "2021-03-09T14:01:57.677385Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:02:11.589959Z",
     "start_time": "2021-03-09T14:02:11.464839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "input_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(input_str)\n",
    "result = [i for i in tokens if not i in stop_words]\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "删除⽂本中出现的稀疏词和特定词\n",
    "\n",
    "在某些情况下，有必要删除⽂本中出现的⼀些稀疏术语或特定词。考虑到任何单词都可以被认为是⼀组\n",
    "终⽌词，因此可以通过终⽌词删除⼯具来实现这⼀⽬标。\n",
    "\n",
    "词⼲提取（Stemming）\n",
    "\n",
    "词⼲提取是⼀个将词语简化为词⼲、词根或词形的过程（如 books-book，looked-look）。当前主流的\n",
    "两种算法是 Porter stemming 算法（删除单词中删除常⻅的形态和拐点结尾） 和 Lancaster stemming\n",
    "算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使⽤ NLYK 实现词⼲提取 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:04:02.875237Z",
     "start_time": "2021-03-09T14:04:02.866261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there\n",
      "are\n",
      "sever\n",
      "type\n",
      "of\n",
      "stem\n",
      "algorithm\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer= PorterStemmer()\n",
    "input_str='There are several types of stemming algorithms.'\n",
    "input_str=word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词形还原（Lemmatization）\n",
    "\n",
    "词形还原的⽬的，如词⼲过程，是将单词的不同形式还原到⼀个常⻅的基础形式。与词⼲提取过程相\n",
    "反，词形还原并不是简单地对单词进⾏切断或变形，⽽是通过使⽤词汇知识库来获得正确的单词形式。\n",
    "当前常⽤的词形还原⼯具库包括： NLTK（WordNet Lemmatizer），spaCy，TextBlob，Pattern，\n",
    "gensim，Stanford CoreNLP，基于内存的浅层解析器（MBSP），Apache OpenNLP，Apache\n",
    "Lucene，⽂本⼯程通⽤架构（GATE），Illinois Lemmatizer 和 DKPro Core。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:05:49.842015Z",
     "start_time": "2021-03-09T14:05:49.831073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "been\n",
      "had\n",
      "done\n",
      "language\n",
      "city\n",
      "mouse\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "input_str='been had done languages cities mice'\n",
    "input_str=word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⽂字处理其他常⽤⽅法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理空格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:06:16.179557Z",
     "start_time": "2021-03-09T14:06:16.173594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 我今天要去打游戏，明天要去吃⽕锅 \n"
     ]
    }
   ],
   "source": [
    "str1=' 我今天要去打游戏，明天要去吃⽕锅 '\n",
    "print(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:06:21.938509Z",
     "start_time": "2021-03-09T14:06:21.933523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我今天要去打游戏，明天要去吃⽕锅\n"
     ]
    }
   ],
   "source": [
    "print(str1.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 去除左右空格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:06:33.827877Z",
     "start_time": "2021-03-09T14:06:33.815947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 我今天要去打游戏，明天要去吃⽕锅\n"
     ]
    }
   ],
   "source": [
    "print(str1.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:06:43.868513Z",
     "start_time": "2021-03-09T14:06:43.859539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我今天要去打游戏，明天要去吃⽕锅 \n"
     ]
    }
   ],
   "source": [
    "print(str1.lstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 去除字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:07:05.282590Z",
     "start_time": "2021-03-09T14:07:05.276607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QQ我今天要去打游戏，明天要去吃⽕锅QQ\n"
     ]
    }
   ],
   "source": [
    "str2='QQ我今天要去打游戏，明天要去吃⽕锅QQ'\n",
    "print(str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:07:11.375394Z",
     "start_time": "2021-03-09T14:07:11.362430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我今天要去打游戏，明天要去吃⽕锅\n"
     ]
    }
   ],
   "source": [
    "print(str2.strip('Q'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:07:13.979698Z",
     "start_time": "2021-03-09T14:07:13.967730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QQ我今天要去打游戏，明天要去吃⽕锅\n",
      "我今天要去打游戏，明天要去吃⽕锅QQ\n"
     ]
    }
   ],
   "source": [
    "print(str2.rstrip('Q'))\n",
    "print(str2.lstrip('Q'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 字符替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:07:24.711893Z",
     "start_time": "2021-03-09T14:07:24.665100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我昨天要去打游戏，明天要去吃⽕锅\n"
     ]
    }
   ],
   "source": [
    "str3='我今天要去打游戏，明天要去吃⽕锅'\n",
    "print(str3.replace('今天','昨天'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 去除某个字符【⽤的很多】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:07:33.418995Z",
     "start_time": "2021-03-09T14:07:33.411049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我要去打游戏，明天要去吃⽕锅\n"
     ]
    }
   ],
   "source": [
    "str4='我今天要去打游戏，明天要去吃⽕锅'\n",
    "print(str3.replace('今天',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分割与合并字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:07:40.748302Z",
     "start_time": "2021-03-09T14:07:40.740323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '今天', '要去', '打', '游戏，明天', '要去', '吃', '⽕锅']\n"
     ]
    }
   ],
   "source": [
    "##分割\n",
    "str5='我 今天 要去 打 游戏，明天 要去 吃 ⽕锅'\n",
    "print(str5.split(' '))\n",
    "splited_str=str5.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:07:46.623031Z",
     "start_time": "2021-03-09T14:07:46.607040Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我 今天 要去 打 游戏，明天 要去 吃 ⽕锅'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##合并\n",
    "' '.join(splited_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:07:57.771345Z",
     "start_time": "2021-03-09T14:07:57.760414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我今天要去打游戏，明天要去吃⽕锅'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(splited_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正则表达式在⽂本挖掘中的基本应⽤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 匹配字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:17:45.825276Z",
     "start_time": "2021-03-09T14:17:45.820328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "利⽤爬⾍技术抓取⽹络信息\n",
      "爬取策略有⼴度爬⾍和深度爬⾍\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text_string=' ⽂本最重要的来源是⽹络。利⽤爬⾍技术抓取⽹络信息。爬取策略有⼴度爬⾍和深度爬⾍。'\n",
    "regex='爬⾍' \n",
    "#查找包含爬⾍两个字的语句\n",
    "for a in text_string.split('。'):\n",
    "    if re.search(regex,a):\n",
    "        print(a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:09:11.148993Z",
     "start_time": "2021-03-09T14:09:11.094338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "利⽤爬⾍技术抓取⽹络信息\n",
      "爬取策略有⼴度爬⾍和深度爬⾍\n"
     ]
    }
   ],
   "source": [
    "#查找包含‘爬’+任意⼀个字的句⼦\n",
    "import re\n",
    "text_string=' ⽂本最重要的来源是⽹络。利⽤爬⾍技术抓取⽹络信息。爬取策略有⼴度爬⾍和深度爬⾍。'\n",
    "for a in text_string.split('。'):\n",
    "    if re.search('爬.',a):\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:09:20.561414Z",
     "start_time": "2021-03-09T14:09:20.552404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⽂本最重要的来源是⽹络\n"
     ]
    }
   ],
   "source": [
    "# 匹配以⽂本开头的句⼦\n",
    "import re\n",
    "text_string='⽂本最重要的来源是⽹络。利⽤爬⾍技术抓取⽹络信息。爬取策略有⼴度爬⾍和深度爬⾍。'\n",
    "for a in text_string.split('。'):\n",
    "    if re.search('^⽂本',a):\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:09:28.423226Z",
     "start_time": "2021-03-09T14:09:28.412261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "爬取策略有⼴度爬⾍和深度爬⾍\n"
     ]
    }
   ],
   "source": [
    "# 匹配以爬⾍结尾的句⼦\n",
    "import re\n",
    "text_string='⽂本最重要的来源是⽹络。利⽤爬⾍技术抓取⽹络信息。爬取策略有⼴度爬⾍和深度爬⾍。'\n",
    "for a in text_string.split('。'):\n",
    "    if re.search('爬⾍$',a):\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:09:53.761370Z",
     "start_time": "2021-03-09T14:09:53.751408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[重要的]今年第七号。\n",
      "[紧要的]中国对印连发强硬信息。\n"
     ]
    }
   ],
   "source": [
    "# 使⽤中括号匹配多个字符\n",
    "import re\n",
    "text_string=['[重要的]今年第七号。','上海发布⻋库销售监管通知。','[紧要的]中国对印连发强硬信息。']\n",
    "for a in text_string:\n",
    "    if re.search('^\\[[重紧]..\\]',a):\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 抽取⽂本中的数字"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过正则表达式匹配年份"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0-9]代表从0到9的所有数字； [a-z]表示从a到z的所有⼩写字⺟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:10:01.286350Z",
     "start_time": "2021-03-09T14:10:01.271361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "War of 1812\n",
      "Happy New Year 2016!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "strings=['War of 1812','There are 5280 feet to a mile','Happy New Year 2016!']\n",
    "for s in strings:\n",
    "    if re.search('[1-2][0-9]{3}',s):\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T14:10:03.792169Z",
     "start_time": "2021-03-09T14:10:03.783195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2016', '2017']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "years_strings='2016 was a good year,but 2017 will be better'\n",
    "re.findall('[2][0-9]{3}',years_strings)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
