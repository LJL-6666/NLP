{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T07:58:59.348554Z",
     "start_time": "2021-04-12T07:58:58.963591Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'joblib' from 'sklearn.externals' (E:\\Anaconda3\\anaconda\\lib\\site-packages\\sklearn\\externals\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f074f19bde85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn_crfsuite\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn_crfsuite\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'joblib' from 'sklearn.externals' (E:\\Anaconda3\\anaconda\\lib\\site-packages\\sklearn\\externals\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn.externals import joblib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T08:01:16.438748Z",
     "start_time": "2021-04-12T08:01:16.419705Z"
    }
   },
   "outputs": [],
   "source": [
    "import joblib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T08:19:53.422324Z",
     "start_time": "2021-04-12T08:19:53.403163Z"
    }
   },
   "outputs": [],
   "source": [
    "dir='F:/大三（下）/文本信息处理与应用/命名实体识别课件/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T08:19:57.756920Z",
     "start_time": "2021-04-12T08:19:57.661599Z"
    }
   },
   "outputs": [],
   "source": [
    "class CorpusProcess(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"初始化\"\"\"\n",
    "        self.train_corpus_path = dir+ \"1980_01rmrb.txt\"\n",
    "        self.process_corpus_path = dir + \"result-rmrb.txt\"\n",
    "        self._maps = {u't': u'T',u'nr': u'PER', u'ns': u'ORG',u'nt': u'LOC'}\n",
    "        \n",
    "    def read_corpus_from_file(self, file_path):\n",
    "        \"\"\"读取语料\"\"\"\n",
    "        f = open(file_path, 'r',encoding='utf-8')\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        return lines\n",
    "    \n",
    "    def write_corpus_to_file(self, data, file_path):\n",
    "        \"\"\"写语料\"\"\"\n",
    "        f = open(file_path, 'wb')\n",
    "        f.write(data)\n",
    "        f.close()\n",
    "        \n",
    "    def q_to_b(self,q_str):\n",
    "        \"\"\"全角转半角\"\"\"\n",
    "        b_str = \"\"\n",
    "        for uchar in q_str:\n",
    "            inside_code = ord(uchar)\n",
    "            if inside_code == 12288:  # 全角空格直接转换\n",
    "                inside_code = 32\n",
    "            elif 65374 >= inside_code >= 65281:  # 全角字符（除空格）根据关系转化\n",
    "                inside_code -= 65248\n",
    "            b_str += chr(inside_code)\n",
    "        return b_str\n",
    "    \n",
    "    def b_to_q(self,b_str):\n",
    "        \"\"\"半角转全角\"\"\"\n",
    "        q_str = \"\"\n",
    "        for uchar in b_str:\n",
    "            inside_code = ord(uchar)\n",
    "            if inside_code == 32:  # 半角空格直接转化\n",
    "                inside_code = 12288\n",
    "            elif 126 >= inside_code >= 32:  # 半角字符（除空格）根据关系转化\n",
    "                inside_code += 65248\n",
    "            q_str += chr(inside_code)\n",
    "        return q_str\n",
    "    \n",
    "    def pre_process(self):\n",
    "        \"\"\"语料预处理 \"\"\"\n",
    "        lines = self.read_corpus_from_file(self.train_corpus_path)\n",
    "        new_lines = []\n",
    "        for line in lines:\n",
    "            words = self.q_to_b(line.strip()).split(u'  ')\n",
    "            pro_words = self.process_t(words)\n",
    "            pro_words = self.process_nr(pro_words)\n",
    "            pro_words = self.process_k(pro_words)\n",
    "            new_lines.append('  '.join(pro_words[1:]))\n",
    "        self.write_corpus_to_file(data='\\n'.join(new_lines).encode('utf-8'), file_path=self.process_corpus_path)\n",
    "    \n",
    "    def process_k(self, words):\n",
    "        \"\"\"处理大粒度分词,合并语料库中括号中的大粒度分词,类似：[国家/n  环保局/n]nt \"\"\"\n",
    "        pro_words = []\n",
    "        index = 0\n",
    "        temp = u''\n",
    "        while True:\n",
    "            word = words[index] if index < len(words) else u''\n",
    "            if u'[' in word:\n",
    "                temp += re.sub(pattern=u'/[a-zA-Z]*', repl=u'', string=word.replace(u'[', u''))\n",
    "            elif u']' in word:\n",
    "                w = word.split(u']')\n",
    "                temp += re.sub(pattern=u'/[a-zA-Z]*', repl=u'', string=w[0])\n",
    "                pro_words.append(temp+u'/'+w[1])\n",
    "                temp = u''\n",
    "            elif temp:\n",
    "                temp += re.sub(pattern=u'/[a-zA-Z]*', repl=u'', string=word)\n",
    "            elif word:\n",
    "                pro_words.append(word)\n",
    "            else:\n",
    "                break\n",
    "            index += 1\n",
    "        return pro_words\n",
    "    \n",
    "    def process_nr(self, words):\n",
    "        \"\"\" 处理姓名，合并语料库分开标注的姓和名，类似：温/nr  家宝/nr\"\"\"\n",
    "        pro_words = []\n",
    "        index = 0\n",
    "        while True:\n",
    "            word = words[index] if index < len(words) else u''\n",
    "            if u'/nr' in word:\n",
    "                next_index = index + 1\n",
    "                if next_index < len(words) and u'/nr' in words[next_index]:\n",
    "                    pro_words.append(word.replace(u'/nr', u'') + words[next_index])\n",
    "                    index = next_index\n",
    "                else:\n",
    "                    pro_words.append(word)\n",
    "            elif word:\n",
    "                pro_words.append(word)\n",
    "            else:\n",
    "                break\n",
    "            index += 1\n",
    "        return pro_words\n",
    "    \n",
    "    def process_t(self, words):\n",
    "        \"\"\"处理时间,合并语料库分开标注的时间词，类似： （/w  一九九七年/t  十二月/t  三十一日/t  ）/w   \"\"\"\n",
    "        pro_words = []\n",
    "        index = 0\n",
    "        temp = u''\n",
    "        while True:\n",
    "            word = words[index] if index < len(words) else u''\n",
    "            if u'/t' in word:\n",
    "                temp = temp.replace(u'/t', u'') + word\n",
    "            elif temp:\n",
    "                pro_words.append(temp)\n",
    "                pro_words.append(word)\n",
    "                temp = u''\n",
    "            elif word:\n",
    "                pro_words.append(word)\n",
    "            else:\n",
    "                break\n",
    "            index += 1\n",
    "        return pro_words\n",
    "    \n",
    "    def pos_to_tag(self, p):\n",
    "        \"\"\"由词性提取标签\"\"\"\n",
    "        t = self._maps.get(p, None)\n",
    "        return t if t else u'O'\n",
    "    \n",
    "    def tag_perform(self, tag, index):\n",
    "        \"\"\"标签使用BIO模式\"\"\"\n",
    "        if index == 0 and tag != u'O':\n",
    "            return u'B_{}'.format(tag)\n",
    "        elif tag != u'O':\n",
    "            return u'I_{}'.format(tag)\n",
    "        else:\n",
    "            return tag\n",
    "        \n",
    "    def pos_perform(self, pos):\n",
    "        \"\"\"去除词性携带的标签先验知识\"\"\"\n",
    "        if pos in self._maps.keys() and pos != u't':\n",
    "            return u'n'\n",
    "        else:\n",
    "            return pos\n",
    "        \n",
    "    def initialize(self):\n",
    "        \"\"\"初始化 \"\"\"\n",
    "        lines = self.read_corpus_from_file(self.process_corpus_path)\n",
    "        words_list = [line.strip().split('  ') for line in lines if line.strip()]\n",
    "        del lines\n",
    "        self.init_sequence(words_list)\n",
    "        \n",
    "    def init_sequence(self, words_list):\n",
    "        \"\"\"初始化字序列、词性序列、标记序列 \"\"\"\n",
    "        words_seq = [[word.split(u'/')[0] for word in words] for words in words_list]\n",
    "        pos_seq = [[word.split(u'/')[1] for word in words] for words in words_list]\n",
    "        tag_seq = [[self.pos_to_tag(p) for p in pos] for pos in pos_seq]\n",
    "        self.pos_seq = [[[pos_seq[index][i] for _ in range(len(words_seq[index][i]))]\n",
    "                        for i in range(len(pos_seq[index]))] for index in range(len(pos_seq))]\n",
    "        self.tag_seq = [[[self.tag_perform(tag_seq[index][i], w) for w in range(len(words_seq[index][i]))]\n",
    "                        for i in range(len(tag_seq[index]))] for index in range(len(tag_seq))]\n",
    "        self.pos_seq = [[u'un']+[self.pos_perform(p) for pos in pos_seq for p in pos]+[u'un'] for pos_seq in self.pos_seq]\n",
    "        self.tag_seq = [[t for tag in tag_seq for t in tag] for tag_seq in self.tag_seq]\n",
    "        self.word_seq = [[u'<BOS>']+[w for word in word_seq for w in word]+[u'<EOS>'] for word_seq in words_seq]   \n",
    "        \n",
    "    def extract_feature(self, word_grams):\n",
    "        \"\"\"特征选取\"\"\"\n",
    "        features, feature_list = [], []\n",
    "        for index in range(len(word_grams)):\n",
    "            for i in range(len(word_grams[index])):\n",
    "                word_gram = word_grams[index][i]\n",
    "                feature = {u'w-1': word_gram[0], u'w': word_gram[1], u'w+1': word_gram[2],\n",
    "                           u'w-1:w': word_gram[0]+word_gram[1], u'w:w+1': word_gram[1]+word_gram[2],\n",
    "                           # u'p-1': self.pos_seq[index][i], u'p': self.pos_seq[index][i+1],\n",
    "                           # u'p+1': self.pos_seq[index][i+2],\n",
    "                           # u'p-1:p': self.pos_seq[index][i]+self.pos_seq[index][i+1],\n",
    "                           # u'p:p+1': self.pos_seq[index][i+1]+self.pos_seq[index][i+2],\n",
    "                           u'bias': 1.0}\n",
    "                feature_list.append(feature)\n",
    "            features.append(feature_list)\n",
    "            feature_list = []\n",
    "        return features \n",
    "    \n",
    "    def segment_by_window(self, words_list=None, window=3):\n",
    "        \"\"\"窗口切分\"\"\"\n",
    "        words = []\n",
    "        begin, end = 0, window\n",
    "        for _ in range(1, len(words_list)):\n",
    "            if end > len(words_list): break\n",
    "            words.append(words_list[begin:end])\n",
    "            begin = begin + 1\n",
    "            end = end + 1\n",
    "        return words\n",
    "    \n",
    "    def generator(self):\n",
    "        \"\"\"训练数据\"\"\"\n",
    "        word_grams = [self.segment_by_window(word_list) for word_list in self.word_seq]\n",
    "        features = self.extract_feature(word_grams)\n",
    "        return features, self.tag_seq\n",
    "    \n",
    "class CRF_NER(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"初始化参数\"\"\"\n",
    "        self.algorithm = \"lbfgs\"\n",
    "        self.c1 =\"0.1\"\n",
    "        self.c2 = \"0.1\"\n",
    "        self.max_iterations = 100\n",
    "        self.model_path = dir + \"model.pkl\"\n",
    "        self.corpus = CorpusProcess()  #Corpus 实例\n",
    "        self.corpus.pre_process()  #语料预处理\n",
    "        self.corpus.initialize()  #初始化语料\n",
    "        self.model = None\n",
    "\n",
    "    def initialize_model(self):\n",
    "        \"\"\"初始化\"\"\"\n",
    "        algorithm = self.algorithm\n",
    "        c1 = float(self.c1)\n",
    "        c2 = float(self.c2)\n",
    "        max_iterations = int(self.max_iterations)\n",
    "        self.model = sklearn_crfsuite.CRF(algorithm=algorithm, c1=c1, c2=c2,\n",
    "                                          max_iterations=max_iterations, all_possible_transitions=True)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"训练\"\"\"\n",
    "        self.initialize_model()\n",
    "        x, y = self.corpus.generator()\n",
    "        x_train, y_train = x[500:], y[500:]\n",
    "        x_test, y_test = x[:500], y[:500]\n",
    "        self.model.fit(x_train, y_train)\n",
    "        labels = list(self.model.classes_)\n",
    "        labels.remove('O')\n",
    "        y_predict = self.model.predict(x_test)\n",
    "        metrics.flat_f1_score(y_test, y_predict, average='weighted', labels=labels)\n",
    "        sorted_labels = sorted(labels, key=lambda name: (name[1:], name[0]))\n",
    "        print(metrics.flat_classification_report(y_test, y_predict, labels=sorted_labels, digits=3))\n",
    "        self.save_model()\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        \"\"\"预测\"\"\"\n",
    "        self.load_model()\n",
    "        u_sent = self.corpus.q_to_b(sentence)\n",
    "        word_lists = [[u'<BOS>']+[c for c in u_sent]+[u'<EOS>']]\n",
    "        word_grams = [self.corpus.segment_by_window(word_list) for word_list in word_lists]\n",
    "        features = self.corpus.extract_feature(word_grams)\n",
    "        y_predict = self.model.predict(features)\n",
    "        entity = u''\n",
    "        for index in range(len(y_predict[0])):\n",
    "            if y_predict[0][index] != u'O':\n",
    "                if index > 0 and y_predict[0][index][-1] != y_predict[0][index-1][-1]:\n",
    "                    entity += u' '\n",
    "                entity += u_sent[index]\n",
    "            elif entity[-1] != u' ':\n",
    "                entity += u' '\n",
    "        return entity\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"加载模型 \"\"\"\n",
    "        self.model = joblib.load(self.model_path)\n",
    "\n",
    "    def save_model(self):\n",
    "        \"\"\"保存模型\"\"\"\n",
    "        joblib.dump(self.model, self.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T08:20:19.354504Z",
     "start_time": "2021-04-12T08:20:01.970742Z"
    }
   },
   "outputs": [],
   "source": [
    "ner = CRF_NER()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T08:26:03.220477Z",
     "start_time": "2021-04-12T08:20:21.736091Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass labels=['B_LOC', 'I_LOC', 'B_ORG', 'I_ORG', 'B_PER', 'I_PER', 'B_T', 'I_T'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B_LOC      0.944     0.827     0.882       266\n",
      "       I_LOC      0.892     0.801     0.844      1203\n",
      "       B_ORG      0.941     0.913     0.927       682\n",
      "       I_ORG      0.932     0.869     0.899       997\n",
      "       B_PER      0.985     0.918     0.951       440\n",
      "       I_PER      0.983     0.939     0.961       824\n",
      "         B_T      0.993     0.993     0.993       444\n",
      "         I_T      0.995     0.995     0.995      1099\n",
      "\n",
      "   micro avg      0.954     0.904     0.929      5955\n",
      "   macro avg      0.958     0.907     0.931      5955\n",
      "weighted avg      0.953     0.904     0.928      5955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = ner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T08:26:09.937956Z",
     "start_time": "2021-04-12T08:26:09.841103Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'新华社 北京 十二月三十一日  中央人民广播电台  刘振英  新华社  张宿堂  今天  一九九七年 '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.predict(u'新华社北京十二月三十一日电(中央人民广播电台记者刘振英、新华社记者张宿堂)今天是一九九七年的最后一天。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T08:26:10.987402Z",
     "start_time": "2021-04-12T08:26:10.920736Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'一九四九年  国庆节  毛泽东  天安门城  中国共产党 '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.predict(u'一九四九年，国庆节，毛泽东同志在天安门城楼上宣布中国共产党从此站起来了！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T08:26:11.741165Z",
     "start_time": "2021-04-12T08:26:11.679156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'王之  林彪 '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.predict(u'王之同志因受林彪四人帮极左路线的迫害')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
